## Local Response Normalization
参考:
- [What Is Local Response Normalization In Convolutional Neural Networks](https://prateekvjoshi.com/2016/04/05/what-is-local-response-normalization-in-convolutional-neural-networks/)
- Alex的论文 [ImageNet Classification with Deep Convolutional Neural Networks ](http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf) 3.3节


## Batch Normalization
- 使得深度神经网络训练更加稳定。
- 加快了收敛速度。
- 正则化。

### 动机
- 很深的网络很难训练
- sigmoid激活函数有饱和和梯度消失的问题。RELU可以缓解这一情况，但是又有`internal covariate shift`的问题。Batch Normalization 可以解决这一问题。
